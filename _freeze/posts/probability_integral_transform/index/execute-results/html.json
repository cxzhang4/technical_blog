{
  "hash": "f44f868d60af1d361b9b227b9f8fefb7",
  "result": {
    "markdown": "---\ntitle: \"Probability integral transform\"\ndescription: \"The probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. I give some intuition for this statement.\"\nauthor: \"Carson Zhang\"\ndate: \"12/04/2023\"\ndraft: false\n---\n\n\nThe probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. This result underlies inverse transform sampling. It is also central to why p-values are normally distributed under the null hypothesis. But why does this make sense?\n\nSuppose we have a random variable $X$ from an arbitrary probability distribution.\n\nHere, $X \\sim \\text{Beta}(\\alpha = 0.9, \\beta = 3.4)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha_x = 0.9\nbeta_x = 3.4\nx_seq <- seq(0, 1, length = 100)\nx_density <- dbeta(x_seq, alpha_x, beta_x)\n\nplot(x_seq, x_density, type = \"l\", lty = 1,\n     xlab = \"X\", ylab = \"Density\", main = \"Density of X\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWhat does $Y = F_X(x)$ look like?\n\nLet's try to draw the density of $Y$ one section at a time.\n\nFirst, suppose we select the top 3% of the distribution. [(This comprises the values between the $0.97$ and $1$ $p$-quantiles of this distribution.)](https://en.wikipedia.org/wiki/Quantile)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile_0.97 <- qbeta(0.97, alpha_x, beta_x)\nquantile_1 <- 1\n\nplot(x_seq, x_density, type = \"l\", lty = 1,\n     xlab = \"X\", ylab = \"Density\", main = \"Density of X\")\nabline(v = c(quantile_0.97, quantile_1), col = \"orange\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe orange lines bound the top 3%.\n\nFor now, since we don't know what the density of $Y = F_X(X)$ looks like, let's say it's an arbitrary curve.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nHowever, since we selected the top 3% of the probability mass, we know that, within the orange interval, the area under the curve must be $0.03$, and therefore the value of the density must be $1$ on average.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNow, think about the region between the $0.97$ and $0.98$ $p$-quantiles of the distribution By definition, this comprises 1% of the probability mass ($0.98 - 0.97 = 0.01$), so we need to adjust our curve to satisfy this condition.\n\nHowever, we note that all intervals have this same property (even arbitrarily small intervals): **the width of each interval is equal to its corresponding probability mass.** So, the density of $Y$ needs to have mean $1$ over any sub-interval of $[0, 1]$.\n\nIt is natural for me to suspect the density of $Y$ to be a horizontal line at $1$: this is the only function I can think of that guarantees this property.\n\nFor extra reading and formalism, we use the above insight to illustrate the theorem.\n\n**Theorem:** $Y = F_X(X) \\sim \\text{Uniform}(0, 1)$.\n\n**Derivation of the density f_Y(Y)**[^1]: \n\nLet $a, b$ be real numbers such that $0 \\leq a < b \\leq 1$.\n\nBy the argument above, we have $F_Y(b) - F_Y(a) = b - a$.\n\n(Note that we can rewrite this as $F(b) = b \\text{ and } F(a) = a$, i.e. $F$ is the identity.)\n\nWe have:\n\n$$\n\\begin{align}\n  F_Y(b) - F_Y(a) &= b - a\\\\\n    &= F_Y(Y) \\Big|_a^b && \\text{(standard antiderivative notation)}\\\\\n    &= \\int_a^b f_Y(y)dy && \\text{(definition of a probability density function)}\\\\\n    &= \\int_a^b 1dy && \\text{(a function with the identity as its antiderivative)}\\\\\n\\end{align}\n$$\n\nSo, we have $f_Y(y) = 1$, and therefore, $Y = F_X(x)$ has the standard uniform distribution.\n\n## P-value distribution under $H_0$\n\nThe p-value of a test statistic $T(X)$ for a one-sided test where the alternative \"is greater\" is\n$P_{H_0}(T \\geq t(x))$.\n\nWe can write\n\n$$\n\\begin{align}\nP_{H_0}(T \\geq t(x)) &= 1 - P_{H_0}(T \\leq t(x))\\\\\n  &= 1 - F_{T; H_0}(T)\\\\\n  &= 1 - Y && \\text{where } Y \\sim U(0, 1)\\\\\n\\end{align}\n$$\n\nAnd we know $1 - Y \\sim U(0, 1)$, so $P_{H_0}(T \\geq t(x)) \\sim U(0, 1)$, and we have shown that p-values are uniformly distributed under the null hypothesis.\n\n[^1]: This feels like it could be a circular argument to me. It's also not entirely necessary: once we know $F_Y(Y)$, we know the distribution of $Y$. I still found it instructive to do work through these steps.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}