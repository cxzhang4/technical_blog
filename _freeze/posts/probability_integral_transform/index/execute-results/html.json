{
  "hash": "7c42851fb06c6e2984e692dbd42853ab",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability integral transform\"\ndescription: \"The probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. I give some intuition for this statement.\"\nauthor: \"Carson Zhang\"\ndate: \"12/04/2023\"\ndraft: false\n---\n\n\nThe probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $\\text{Uniform}(0, 1)$. This result underlies inverse transform sampling. It illustrates why p-values are uniformly distributed under the null hypothesis. It is central to how copulas can model joint distributions. But why does this make sense?\n\nSuppose we have a random variable $X$ from an arbitrary probability distribution.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWhat does $Y = F_X(x)$ look like?\n\nLet's try to draw the pdf of $Y$ one section at a time.\n\nFirst, suppose we select the top 3% of the distribution, [(i.e. values between the $0.97$ and $1$ -quantiles of this distribution.)](https://en.wikipedia.org/wiki/Quantile)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe orange lines bound the top 3%.\n\nFor now, since we don't know what the density of $Y = F_X(X)$ looks like, let's say it's an arbitrary curve.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nHowever, recall that we selected the top 3% of the probability mass, so within the orange interval, the area under the curve must be $0.03$, and therefore the value of the pdf must be $1$ on average within the orange interval.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNow, think about the region between the $0.97$ and $0.98$-quantiles of the distribution. By definition, this comprises 1% of the probability mass ($0.98 - 0.97 = 0.01$), so we need to adjust our curve to satisfy this condition.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nHowever, we note that all intervals have this same property (even arbitrarily small intervals): **the width of each interval is equal to its corresponding probability mass.** So, the pdf of $Y$ needs to have mean $1$ over any sub-interval of $[0, 1]$, no matter its size or location.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nIt is natural for me to suspect the pdf of $Y$ to be a horizontal line at $1$: this is the only function I can think of that guarantees this property.\n\nWe use the above insight to illustrate the theorem.\n\n\n\n**Theorem (Probability Integral Transform):** $Y = F_X(X) \\sim \\text{Uniform}(0, 1)$.\n\n**Proof**: the standard proof of the PIT found on the [Wikipedia page](https://en.wikipedia.org/wiki/Probability_integral_transform).\n\n$$\n\\begin{align}\n  F_Y(y) &= P(Y \\leq y)\\\\\n    &= P(F_X(X) \\leq y) && \\text{(substituted the definition of } Y)\\\\\n    &= P(X \\leq F_X^{-1}(y)) && \\text{(applied } F_X^{-1} \\text{ to both sides)}\\\\\n    &= F_X(F_X^{-1}(y)) && \\text{(the definition of a CDF)}\\\\\n    &= y\n\\end{align}\n$$\n\nTherefore, $Y \\sim \\text{Uniform}(0, 1)$.\n\n## P-value distribution under $H_0$ [^2]\n\nThe p-value of a test statistic $T(X)$ for a one-sided test where the alternative \"is greater than\" is\n$P_{H_0}(T \\geq t(x))$. We can already see that this looks like a CDF (the \"less than\" alternative really is just a CDF), so our above insights will hold.\n\nDefine $P_{greater} := \\Pr_{H_0}(T \\geq t(x)) = 1 - F_{T; H_0}(T)$.\n\n\n$$\n\\begin{align}\nF_{P_{\\text{greater}}}(p) &= \\Pr(P_{greater} \\leq p) && \\text{(definition of a CDF)}\\\\\n  &= \\Pr((1 - F_{T; H_0}(T)) \\leq p)\\\\\n  &= \\Pr(-F_{T; H_0}(T) \\leq (p - 1))\\\\\n  &= \\Pr(F_{T; H_0}(T) \\geq (1 - p))\\\\\n  &= 1 - \\Pr(F_{T; H_0}(T) \\leq (1 - p))\\\\\n  &= 1 - \\Pr(T \\leq F_{T; H_0}^{-1}(1 - p)) && \\text{(applied } F_X^{-1} \\text{ to both sides)}\\\\\n  &= 1 - F_{T; H_0}(F_{T; H_0}^{-1}(1 - p)) && \\text{(definition of a CDF)}\\\\\n  &= 1 - (1 - p)\\\\\n  &= p\\\\\n  &= F_{U(0, 1)}(p)\n\\end{align}\n$$\n\nThus, we have shown that one-sided p-values are uniformly distributed under the null hypothesis.[^3]\n\n## Acknowledgements\n\nThank you to Meimingwei Li, Raphael Rehms, Dr. Fabian Scheipl, Prof. Michael Schomaker, and J.P. Weideman for their helpful input.\n\n[^1]: This is not necessary: once we know $F_Y(Y)$, we know the distribution of $Y$. I'm also not convinced that this is a rigorous derivation. I still found it instructive to work through these steps.\n\n[^2]: Notation and proof from and inspired by Raphael Rehms's exercise and solution from the Statistical Methods in Epidemiology course.\n\n[^3]: [This holds only for divergence p-values, not decision p-values](https://arxiv.org/abs/2301.02478). My understanding is that divergence p-values are one-sided p-values. Thanks to Prof. Schomaker for this insight.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}