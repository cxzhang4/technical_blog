---
title: "Probability integral transform"
description: "The probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. I give some intuition for this statement."
author: "Carson Zhang"
date: "12/04/2023"
draft: false
---

The probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. This result underlies inverse transform sampling. It illustrates why p-values are uniformly distributed under the null hypothesis. It is central to how copulas can model joint distributions. But why does this make sense?

Suppose we have a random variable $X$ from an arbitrary probability distribution.

Here, $X \sim \text{Beta}(\alpha = 0.9, \beta = 3.4)$

```{r}
alpha_x = 0.9
beta_x = 3.4
x_seq <- seq(0, 1, length = 100)
x_density <- dbeta(x_seq, alpha_x, beta_x)

plot(x_seq, x_density, type = "l", lty = 1,
     xlab = "X", ylab = "Density", main = "Density of X")
```

What does $Y = F_X(x)$ look like?

Let's try to draw the pdf of $Y$ one section at a time.

First, suppose we select the top 3% of the distribution. [(This comprises the values between the $0.97$ and $1$ $p$-quantiles of this distribution.)](https://en.wikipedia.org/wiki/Quantile)

```{r}
quantile_0.97 <- qbeta(0.97, alpha_x, beta_x)
quantile_1 <- 1

plot(x_seq, x_density, type = "l", lty = 1,
     xlab = "X", ylab = "Density", main = "Density of X")
abline(v = c(quantile_0.97, quantile_1), col = "orange")
```

The orange lines bound the top 3%.

For now, since we don't know what the density of $Y = F_X(X)$ looks like, let's say it's an arbitrary curve.

```{r echo=FALSE}
cdf_seq <- seq(0, 1, length = 10000)

cdf_arbitrary_guess <- (1/4) * sin(2 * pi * cdf_seq) + 1

plot(cdf_seq, cdf_arbitrary_guess, type = "l", 
     xlab = "Y = F(X)", ylab = "Density", main = "Unknown density of Y = F(X)",
     xlim = c(0, 1), ylim = c(0, 1.5))
abline(v = c(0.97, 1), col = "orange")
```

However, recall that we selected the top 3% of the probability mass, so within the orange interval, the area under the curve must be $0.03$, and therefore the value of the pdf must be $1$ on average within the orange interval.

```{r echo=FALSE}
cdf_seq_orange <- seq(0.97, 1, by = 0.0001)
cdf_density_orange <- (1/4) * sin(2 * pi / 0.015 * (cdf_seq_orange - 0.97)) + 1

plot(cdf_seq_orange, cdf_density_orange, type = "l", xlim = c(0, 1), ylim = c(0, 1.5),
     main = "The top 3% of the unknown density of Y", xlab = "Value of the CDF of X", ylab = "Density")
abline(h = mean(cdf_density_orange), col = "orange", lty = "dashed")
abline(v = c(0.97, 1), col = "orange")
```

Now, think about the region between the $0.97$ and $0.98$ $p$-quantiles of the distribution By definition, this comprises 1% of the probability mass ($0.98 - 0.97 = 0.01$), so we need to adjust our curve to satisfy this condition.

However, we note that all intervals have this same property (even arbitrarily small intervals): **the width of each interval is equal to its corresponding probability mass.** So, the pdf of $Y$ needs to have mean $1$ over any sub-interval of $[0, 1]$.

It is natural for me to suspect the pdf of $Y$ to be a horizontal line at $1$: this is the only function I can think of that guarantees this property.

For extra reading and formalism, we use the above insight to illustrate the theorem.

**Theorem (Probability Integral Transform):** $Y = F_X(X) \sim \text{Uniform}(0, 1)$.

**Derivation of the pdf $f_Y(Y)$**[^1]: A "proof" that the pdf of $Y = F_X(X)$ is $1$, starting from the insight given above.

Let $a, b$ be real numbers such that $0 \leq a < b \leq 1$.

By the argument above, we have $F_Y(b) - F_Y(a) = b - a$.

(Note that we can rewrite this as $F(b) = b \text{ and } F(a) = a$, i.e. $F$ is the identity.)

We have:

$$
\begin{align}
  F_Y(b) - F_Y(a) &= b - a\\
    &= F_Y(Y) \Big|_a^b && \text{(standard antiderivative notation)}\\
    &= \int_a^b f_Y(y)dy && \text{(definition of a probability density function)}\\
    &= \int_a^b 1dy && \text{(a function with the identity as its antiderivative)}\\
\end{align}
$$

So, we have $f_Y(y) = 1$, and therefore, $Y = F_X(x)$ has the standard uniform distribution.

**Proof**: the standard proof of the PIT found on the [Wikipedia page](https://en.wikipedia.org/wiki/Probability_integral_transform).

$$
\begin{align}
  F_Y(y) &= P(Y \leq y)\\
    &= P(F_X(X) \leq y) && \text{(substituted the definition of } Y)\\
    &= P(X \leq F_X^{-1}(y)) && \text{(applied } F_X^{-1} \text{ to both sides)}\\
    &= F_X(F_X^{-1}(y)) && \text{(the definition of a CDF)}\\
    &= y
\end{align}
$$

Therefore, $Y \sim U(0, 1)$.

## P-value distribution under $H_0$

The p-value of a test statistic $T(X)$ for a one-sided test where the alternative "is greater" is
$P_{H_0}(T \geq t(x))$.

We can write

$$
\begin{align}
P_{H_0}(T \geq t(x)) &= 1 - P_{H_0}(T \leq t(x))\\
  &= 1 - F_{T; H_0}(T)\\    
  &= 1 - F_{U(0, 1)} && \text{(probability integral transform)}\\
  &= F_{U(0, 1)}
\end{align}
$$

Since $F_{T; H_0}(T)$ is a random variable's CDF applied to itself, its distribution is standard uniform by the probability integral transform. Therefore, $1 - F_{T; H_0}(T) \sim U(0, 1)$, and we have shown that p-values are uniformly distributed under the null hypothesis.[^2]

## Acknowledgements

Thank you to Meimingwei Li, Raphael Rehms, Prof. Michael Schomaker, and J.P. Weideman for their helpful input.


[^1]: This is not necessary: once we know $F_Y(Y)$, we know the distribution of $Y$. I'm also not entirely convinced that this is a rigorous derivation. I still found it instructive to work through these steps.

[^2]: [This holds only for divergence p-values, not decision p-values](https://arxiv.org/abs/2301.02478). Thanks again to Prof. Schomaker for this insight, and for bringing this to my attention.