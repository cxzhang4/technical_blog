---
title: "Probability integral transform"
description: "The probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. I give some intuition for this statement."
author: "Carson Zhang"
date: "12/04/2023"
draft: false
---

The probability integral transform states that, for a continuous random variable $X$, the distribution of $Y = F_X(X)$ is $U(0, 1)$. This result underlies inverse transform sampling. It is also central to why p-values are normally distributed under the null hypothesis. But why does this make sense?

Suppose we have a random variable $X$ from an arbitrary probability distribution.

Here, $X \sim \text{Beta}(\alpha = 0.9, \beta = 3.4)$

```{r}
alpha_x = 0.9
beta_x = 3.4
x_seq <- seq(0, 1, length = 100)
x_density <- dbeta(x_seq, alpha_x, beta_x)

plot(x_seq, x_density, type = "l", lty = 1,
     xlab = "X", ylab = "Density", main = "Density of X")
```

What does $Y = F_X(x)$ look like?

Let's try to draw the density of $Y$ one section at a time.

First, suppose we select the top 3% of the distribution. [(This comprises the values between the $0.97$ and $1$ $p$-quantiles of this distribution.)](https://en.wikipedia.org/wiki/Quantile)

```{r}
quantile_0.97 <- qbeta(0.97, alpha_x, beta_x)
quantile_1 <- 1

plot(x_seq, x_density, type = "l", lty = 1,
     xlab = "X", ylab = "Density", main = "Density of X")
abline(v = c(quantile_0.97, quantile_1), col = "orange")
```

The orange lines bound the top 3%.

For now, since we don't know what the density of $Y = F_X(X)$ looks like, let's say it's an arbitrary curve.

```{r echo=FALSE}
cdf_seq <- seq(0, 1, length = 10000)

cdf_arbitrary_guess <- (1/4) * sin(2 * pi * cdf_seq) + 1

plot(cdf_seq, cdf_arbitrary_guess, type = "l", 
     xlab = "Y = F(X)", ylab = "Density", main = "Unknown density of Y = F(X)",
     xlim = c(0, 1), ylim = c(0, 1.5))
abline(v = c(0.97, 1), col = "orange")
```

However, since we selected the top 3% of the probability mass, we know that, within the orange interval, the area under the curve must be $0.03$, and therefore the value of the density must be $1$ on average.

```{r echo=FALSE}
cdf_seq_orange <- seq(0.97, 1, by = 0.0001)
cdf_density_orange <- (1/4) * sin(2 * pi / 0.015 * (cdf_seq_orange - 0.97)) + 1

plot(cdf_seq_orange, cdf_density_orange, type = "l", xlim = c(0, 1), ylim = c(0, 1.5),
     main = "The top 3% of the unknown density of Y", xlab = "Value of the CDF of X", ylab = "Density")
abline(h = mean(cdf_density_orange), col = "orange", lty = "dashed")
abline(v = c(0.97, 1), col = "orange")
```

Now, think about the region between the $0.97$ and $0.98$ $p$-quantiles of the distribution By definition, this comprises 1% of the probability mass ($0.98 - 0.97 = 0.01$), so we need to adjust our curve to satisfy this condition.

However, we note that all intervals have this same property (even arbitrarily small intervals): **the width of each interval is equal to its corresponding probability mass.** So, the density of $Y$ needs to have mean $1$ over any sub-interval of $[0, 1]$.

It is natural for me to suspect the density of $Y$ to be a horizontal line at $1$: this is the only function I can think of that guarantees this property.

For extra reading and formalism, we use the above insight to illustrate the theorem.

**Theorem (Probability Integral Transform):** $Y = F_X(X) \sim \text{Uniform}(0, 1)$.

**Derivation of the density $f_Y(Y)$**[^1]: 

Let $a, b$ be real numbers such that $0 \leq a < b \leq 1$.

By the argument above, we have $F_Y(b) - F_Y(a) = b - a$.

(Note that we can rewrite this as $F(b) = b \text{ and } F(a) = a$, i.e. $F$ is the identity.)

We have:

$$
\begin{align}
  F_Y(b) - F_Y(a) &= b - a\\
    &= F_Y(Y) \Big|_a^b && \text{(standard antiderivative notation)}\\
    &= \int_a^b f_Y(y)dy && \text{(definition of a probability density function)}\\
    &= \int_a^b 1dy && \text{(a function with the identity as its antiderivative)}\\
\end{align}
$$

So, we have $f_Y(y) = 1$, and therefore, $Y = F_X(x)$ has the standard uniform distribution.

## P-value distribution under $H_0$

The p-value of a test statistic $T(X)$ for a one-sided test where the alternative "is greater" is
$P_{H_0}(T \geq t(x))$.

We can write

$$
\begin{align}
P_{H_0}(T \geq t(x)) &= 1 - P_{H_0}(T \leq t(x))\\
  &= 1 - F_{T; H_0}(T)\\
  &= 1 - Y && \text{where } Y \sim U(0, 1)\\
\end{align}
$$

And we know $1 - Y \sim U(0, 1)$, so $P_{H_0}(T \geq t(x)) \sim U(0, 1)$, and we have shown that p-values are uniformly distributed under the null hypothesis.

[^1]: This feels like it could be a circular argument to me. It's also not entirely necessary: once we know $F_Y(Y)$, we know the distribution of $Y$. I still found it instructive to do work through these steps.